=======================
Architecture: Collector
=======================

This document covers the configuration and workings of the part of Socorro known
as "the collector" as it's configured to run on Mozilla's -stage and -prod
environments.

Unofficial sequel to The Fast and the Furious.


Collector requirements
======================

Hand-wavey requirements:

1. **Reliable**: We shouldn't drop crashes and the HTTP endpoint should be
   working--lots of 9s.
2. **Scale horizontally**: We have multiple collector nodes behind an ELB.
3. **Low maintenance**: Should run without kicking.

Hard requirements:

1. **Handle crash reports up to 3mb in size** with enough clearance to continue
   growing. (See crash report size analysis section.)
2. **Generate crash id** which gets returned to the client and also gets added
   to the raw crash.
3. **Save raw crash to S3**
4. **Notify processor** about crash reports to process (currently done with
   RabbitMQ).
5. **Throttle incoming crash reports** so as to sample incoming Firefox desktop
   crash reports so we don't overwhelm the processor as well as drop junk data
   without further processing. [#]_
6. **Feed our -stage system** with real crash data for testing and development. [#]_

.. [#] Telemetry's edge doesn't do this--they do it in later processing. Maybe
       we could do that, too?

.. [#] There are probably other ways to feed our -stage system with real data.


Crash report size analysis
==========================

This is hand-wavey for now since these numbers are from our -stage environment
on crash reports which are siphoned from our -prod environment. Because of how
this mechanism works, this data has the following caveats:

1. it's a 10% sample of crash reports accepted by -prod
2. it doesn't differentiate between crash reports that come in compressed vs.
   uncompressed

Further, since I put the metrics in on August 24th, 2017, this data only covers
a couple of days.

==============  =======
title           amt
==============  =======
max             1.34mb
95 percentile   1.23mb
avg             574kb
median          393kb
==============  =======

FIXME: Acquire new numbers covering at least a week's worth of data.


Collector architecture
======================

Each web node runs three things:

1. nginx
2. uwsgi which manages the WSGI application (socorro-collector service)
3. crashmover process (socorro-crashmover service)


Web app
-------

The web app part of the collector uses the web.py WSGI application framework.

In production, it uses the following prefixes in consul: ``socorro/common``
and ``socorro/collector``.

It handles incoming HTTP POST requests to the ``/submit`` endpoint.

Breakpad crash data is submitted as ``multipart/form-data``.

It can be gzipped.

It can contain a ``Throttleable=1`` parameter which indicates that the crash
should not be throttled.

The nginx ``socorro-collector.conf`` config file in the socorro-infra repository
restricts the maximum body size to 20mb.

Incoming HTTP POST requests are handled in this way:

1. contents are uncompressed if need be
2. ``current_timestamp`` is generated
3. if there's no ``uuid`` parameter, then a crash id is created from a randomly
   generated uuid4 and the date of the current timestamp as YYMMDD
4. the crash is run through the throttler and the result and percentage is
   saved on the raw crash
5. depending on the throttle result:

   ACCEPT, DEFER:

   1. crash is saved to the local filesystem
   2. ``CrashID=xxxxx`` is returned to client in HTTP response

   DISCARD:

   1. ``Discarded=1`` is returned to client

   IGNORED:

   1. ``Unsupported=1`` is returned to client


As of August 5th, 2016, the throttler rules are something like::

  # drop the browser side of all multi submission hang crashes
  ("*", '''lambda d: "HangID" in d
      and d.get("ProcessType", "browser") == "browser"''', None),
  # 100% of crashes with comments
  ("Comments", '''lambda x: x''', 100),
  # 100% of all aurora, beta, esr channels
  ("ReleaseChannel", '''lambda x: x in ("aurora", "beta", "esr")''', 100),
  # 100% of all crashes that report as being nightly
  ("ReleaseChannel", '''lambda x: x.startswith('nightly')''', 100),
  # 10% of Firefox
  ("ProductName", 'Firefox', 10),
  # 100% of Fennec
  ("ProductName", 'Fennec', 100),
  # 100% of all alpha, beta or special
  ("Version", r'''re.compile(r'\..*?[a-zA-Z]+')''', 100),
  # 100% of Thunderbird & SeaMonkey
  ("ProductName", '''lambda x: x[0] in "TSC"''', 100),
  # reject everything else
  (None, True, 0)


Crashmover process
------------------

The crashmover process monitors the local filesystem for new crashes.

In production, it uses the following prefixes in consul: ``socorro/common`` and
``socorro/crashmover``.

For each crash, it does the following:

1. saves the crash to S3 as a "raw_crash"
2. (ACCEPT-only) tosses the crash id in the "socorro.normal" rabbitmq queue for
   processing
3. (PROD-only, ACCEPT-only) tosses the crash id in the "socorro.stagesubmitter"
   rabbitmq queue for processing
4. plus some statsd pings for various things


Architectural things to note
============================

1. nginx can't decompress POST data, so we have to do it in Python-land

2. The ``socorro-collector.conf`` nginx conf file limits HTTP body data to
   20mb maximum. Thus the maximum crash report size would be whatever we can
   compress into 20mb.

3. We want to return a crashid and end the HTTP connection as quickly as
   possible. Because of this, we can't wait to send the data to S3 and RabbitMQ.
   Thus we store the crash on disk and have the separate crashmover process deal
   with it.

4. Storing the crash on disk allows us to manually go in and send crashes along
   if the crashmover process ever dies and can't come back up.

5. We want to be able to get a list of all crashes that came in on a specific
   day. Because of that, we use the following pseudo-filename schema::

     {prefix}/v2/{name_of_thing}/{entropy}/{date}/{id}

   For the Mozilla production setup, that's effectively::

     /v2/raw_crash/{entropy}/{date}/{id}

   where "entropy" is the first three characters of the id and "date" is the last
   six characters.

6. We siphon 10% of crashes submitted to the production system to the stage
   system. The way we do this is by having the production collector crashmover
   submit 10% of incoming crashes to the ``socorro.stag esubmitter`` rabbitmq
   queue.

   A magical fairy named "stage submitter" watches that queue, pulls the raw
   crash data from S3 and HTTP POSTs it to the stage collector.

7. We remove ``\00`` characters from incoming crash data because it hoses later
   processing. Theoretically, there shouldn't be any in there anyhow.
